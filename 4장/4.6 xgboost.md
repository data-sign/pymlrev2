# Xgboost
- C/C++ 로 작성된 XGBoost 핵심 라이브러리를 호출하여 파이썬에서 구동 가능하도록 만든 파이썬 패키지 xgboost 
- 사이킷런과 연동할 수 있도록 래퍼 클래스 XGBClassifier, XGBRegressor 제공
- 해당 모듈로 사이킷런의 아키텍처를 적용하여 fit(), predict() 메서드와 다양한 유틸릴티(cross_val_score, GridSearchCV, Pipeline) 사용 가능 

## 장점
1. 분류와 회귀에 있어 뛰어난 예측 성능
2. GBM 대비 빠른 수행 시간 
- 기존 GBM이 weak learner가 가중치를 증감하는 방법으로 학습하여 학습 속도가 느린 단점을 병렬 수행 및 다양한 기능으로 보완 
3. 과적합 규제 (Regularization) 
- 표준 GBM에는 없는 과적합 규제 기능 존재 
4. 나무 가지치기 (Tree pruning) 
- GBM : 분할 시 부정손실 발생하면 분할 수행 x 
- XGBoost : 더 이상 긍정 이득이 없는 분할을 가지치기 해서 분할 수를 줄임 
5. 자체 내장된 교차 검증 
- 반복 수행 시마다 내부적으로 학습 데이터 세트와 펴가 데이터 세트에 대한 교차 검증 수행하여 반복 수행 횟수를 최적화 
- 교차 검증으로 최적화하여 반복을 중간에 멈추는 조기 중단 기능 존재 
6. 결손값 자체 처리 

## 설치 
```bash
(window) conda install -c anaconda py-xgboost
(linux) conda install -c conda-forge xgboost
```
```python
## 설치 확인
import xgboost as xgb
from xgboost import XGBClassifier
```
## 파이썬 래퍼 XGBoost 하이퍼 파라미터 
- GBM과 유사한 하이퍼 파라미터를 지니며, 조기 중단, 과적합 규제를 위한 하이퍼 파라미터 추가 됨 
- 파이썬 래퍼 XGBoost 모듈과 사이킷런 래퍼 XGBooost 모듈의 일부 하이퍼파라미터는 기능은 같으나 파라미터 명이 다르므로 주의 필요 
- 하이퍼 파라미터는 크게 일반 파라미터, 부스터 파라미터, 학습 태스크 파라미터 3가지 유형으로 나뉨
    - 일반 파라미터 : 일반적으로 실행 시 스레드의 개수나 silent 모드 등의 파라미터
    - 부스터 파라미터 : 트리 최적화, 부스팅, regularizaion 등의 파라미터 
    - 학습 태스크 파라미터 : 학습 수행 시의 객체 함수, 평가를 위한 지표 등을 설정하는 파라미터
### 주요 일반 파라미터 
- booster : gbtree(tree based model, default), gblinear(linear model)
- silent : 0(defualt), 출력 메시지 표시하지 않으려면 1
- nthread : CPU 실행 스레드 개수 조정. default는 전체 스레드 사용 
### 주요 부스터 파라미터 
- eta [default=0.3, alias:learning_rate] : GBM 학습률과 같은 파라미터로 0-1 사이 값으로 지정하며 부스팅 스텝을 반복적으로 수행할 때 업데이트되는 학습률 값. 파이썬 기반의 default는 0.3, 사이킷런 래퍼 클래스를 이용할 때 파라미터는 learning_rate로 대체되고 디폴트는 0.1. 이때, 0.1-0.2사이의 값 추천.
- num_boost_rounds : GBM의 n_estimators 같은 파라미터 
- num_child_weight[default=1]: 트리에서 추가적으로 가지를 나눌지 결정하기 위해 필요한 데이터들의 weight 총합. min_child_weight 클수록 분할 자제. 과적합 조절 위해 사용됨. 
- gamma[default=0, alias:min_split_loss]: 트리의 리프 노드를 추가적으로 나눌지 결정하는 최소 손실 감소 값. 해당 값보다 큰 소실이 감소된 경우 리프 노드 분할. 값이 클수록 과적합 감소 효과 존재 
- max_depth[default=6]: 0 지정 시 깊이 제한 없음. 값이 높으면 특정 피처 조건에 특화되어 룰 조건이 만들어져 과적합 가능성 높아짐. 일반적으로 3~10 사이 값 적용 
- sub_sample[default=1]: 트리가 커져 과적합되는 것을 제어하기 위해 데이터의 샘플링 비율을 지정. 0~1 사이의 값 지정 가능하나 일반적으로 0.5~1 사이 값 사용. 0.5 지정 시 전체 데이터의 절반을 트리 생성하는데 사용. 
- colsample_bytree[default=1]: GBM의 max_features 유사. 트리 생성에 필요한 피처를 임의로 샘플링. 매우 많은 피처가 있는 경우 과적합 조정하는 파라미터. 
-lambda[default=1, alias:reg_lambda]: L2 Regularization 적용 값. 피처 개수가 많을 경우 적용을 검토하며, 값이 클수록 과적합 감소 효과 있음. 
- alpha[default=0, alias:reg_alpha]: L1 Regularization 적용 값. 피처 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과 있음. 
- scale_pos_weight[default=1]: 불균형 데이터 세트의 균형을 유지하기 위한 파라미터 
### 학습 태스크 파라미터 
- objective : 최솟값을 가져야할 손실 함수 지정
    - binary:logistic : 이진 분류일 때 적용
    - multi:softmax: 다중 분류일 때 적용. 레이블 클래스의 개수인 num_class 파라미터 적용 필요 
    - multi:softprob: 개별 레이블 클래스의 예측 확률을 반환 
- eval_metric : 검증에 사용되는 함수 지정. 디폴트는 회귀-rmse, 분류-error. 
    - rmse : Root Mean Square Error
    - mae : Mean Absolute Error
    - logloss : Negative log-likelihood
    - error : Binary classification error rate (0.5 threshold)
    - merror : Multiclass classification error rate 
    - mlogloss : Multiclass logloss
    - auc : Area under the curve 

### 성능 튜닝
- 뛰어난 알고리즘일수록 파라미터 튜닝 영향이 적음
- 데이터의 특성과 상황(피처의 수가 많은 경우, 피처 간 상관성 클 경우 등)에 따라 파라미터 튜닝 방법을 다르게 적용
- 예) 과적합 문제 발생 시 튜닝 방법
    - eta 값을 낮춤(0.01~0.1). eta 값을 낮출 경우 num_round(or n_estimator)는 높여줘야 함
    - max_depth를 낮춤
    - min_child_weight 값 높임
    - gamma 값 높임
    - subsample, colsample_bytree 조정 

       

