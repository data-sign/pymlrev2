## 8.6 토픽모델링
- 토픽모델링 : 문서 집합에 숨어 있는 주제 찾는 것
- BoW에 기반한 DTM이나 TF-IDF는 기본적으로 단어의 빈도 수를 이용한 수치화 방법이기 때문에 단어의 의미를 고려하지 못한다는 단점이 있음
- 이를 보완하여 잠재의미를 분석하는 방법 대두 

### LSA(Latent Semantic Analysis, 잠재의미분석)
- https://wikidocs.net/24949
- https://heeya-stupidbutstudying.tistory.com/57
- DTM이나 TF-IDF 행렬에 절단된 SVD(truncated SVD)를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 추출 
- 차원이 축소된 U, S, VT 해석 
    - 축소된 U: 문서의 개수 × 토픽의 수 t의 크기
    - U : 각 행은 잠재 의미를 표현하기 위한 수치화 된 각각의 문서 벡터
    - 각 문헌이 어떤 잠재 의미군에 속하는 용어들을 많이 포함하고 있는지 알수 있음 
    - 축소된 VT :  토픽의 수 t × 단어의 개수의 크기
    - VT의 각 열은 잠재 의미를 표현하기 위해 수치화된 각각의 단어 벡터
    - 각 잠재 의미군에 어떤 용어들이 속하는지 비중

- 장점
    - 쉽고 빠르게 구현이 가능
    - 단어의 잠재적인 의미를 이끌어낼 수 있어 문서의 유사도 계산 등에서 좋은 성능을 보여줌
- 단점 
    - 이미 계산된 LSA에 새로운 데이터를 추가하여 계산산할 때 보통 처음부터 다시 계산해야 하므로 새로운 정보에 대한 업데이트 어려움


### LDA(Latent Dirichlet Allocation, 잠재 디리클레 할당)
- https://wikidocs.net/30708
- 함께 자주 나타나는 단어의 그룹을 찾는 것
- 각 문서에 토픽의 일부가 혼합되어 있으며, 토픽들은 확률 분포에 기반하여 단어들을 생성한다고 가정
- 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽을 추출
- 예시 

<각 문서의 토픽 분포>
문서1 : 토픽 A 100%
문서2 : 토픽 B 100%
문서3 : 토픽 B 60%, 토픽 A 40%

<각 토픽의 단어 분포>
토픽A : 사과 20%, 바나나 40%, 먹어요 40%, 귀여운 0%, 강아지 0%, 깜찍하고 0%, 좋아요 0%
토픽B : 사과 0%, 바나나 0%, 먹어요 0%, 귀여운 33%, 강아지 33%, 깜찍하고 16%, 좋아요 16%


